{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "implementationDynamicMLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNDa5LqWYTT4p4SHqCSsFEx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/farhadhsn8/MLP/blob/master/implementationDynamicMLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attribute Information:\n",
        "\n",
        "1. sepal length in cm\n",
        "2. sepal width in cm\n",
        "3. petal length in cm\n",
        "4. petal width in cm\n",
        "\n",
        "## class:\n",
        "1. Iris Setosa\n",
        "2. Iris Versicolour\n",
        "3. Iris Virginica"
      ],
      "metadata": {
        "id": "oe3sbQBguksr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "id": "Iky3dAh7qIXZ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "import random\n",
        "import numpy as np\n",
        "import math\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dd = np.array([2,4,1,3,0,1])\n",
        "print(all(dd!=math.inf))"
      ],
      "metadata": {
        "id": "KMG9lt71S3EL",
        "outputId": "149c12b0-65ab-4aa0-88f4-2f48e0b660a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "def load_data():\n",
        "    x1 = numpy.random.randint(-5,-1,100)\n",
        "    y1 = numpy.random.randint(0,10,100,)\n",
        "    # print(y)\n",
        "    data1 = []\n",
        "    for i in range(100):\n",
        "        data1.append(([x1[i],y1[i]]))\n",
        "    # for i in x:\n",
        "    #     numpy.ap(x,y)\n",
        "\n",
        "    x2 = numpy.random.randint(1,5,100)\n",
        "    y2 = numpy.random.randint(0,10,100,)\n",
        "\n",
        "    data2 = []\n",
        "    for i in range(100):\n",
        "        data2.append(([x2[i],y2[i]]))\n",
        "\n",
        "    data = []\n",
        "    for i in range(100):\n",
        "        data.append(data1[i])\n",
        "        data.append(data2[i])\n",
        "\n",
        "    target = []\n",
        "    for i in range(200):\n",
        "        if i % 2 == 0: target.append([1,0])\n",
        "        if i % 2 == 1: target.append([0,1])\n",
        "\n",
        "\n",
        "    dataset = {'data': data, 'target':target}\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "WOm6hT4Vy9VA"
      },
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP:\n",
        "\n",
        "#   PARAMS = {\n",
        "#     # enter learning rate :0.1\n",
        "#     # enter number of layers :3\n",
        "#     # enter code of function for layer0 =>[ 1.sigmoid  | 2.tanh  | 3.relu | 4.linear ] :4\n",
        "#     # enter number of Perceptrons for  layer 1 (start layer number from 0) : 2\n",
        "#     # enter code of function for layer1 =>[ 1.sigmoid  | 2.tanh  | 3.relu | 4.linear ] :4\n",
        "#     # enter code of function for layer2 =>[ 1.sigmoid  | 2.tanh  | 3.relu | 4.linear ] :4\n",
        "\n",
        "#     'LEARNING_RATE' : 0.1 ,\n",
        "#     'NUMBER_OF_LAYRS' : 3 ,\n",
        "#     'CODE_OF_ACTIVATION_FUNCTIONS' : [4,4,4] ,\n",
        "#     'NUMBER_OF_PERCEPTRONS_FOR_HIDDEN_LAYERS' : [2]\n",
        "#  }\n",
        "\n",
        "  def __init__(self , feacturesOftrainingdata , lablesOftrainingdata , parameters):\n",
        "    self.parameters = parameters\n",
        "    self.feacturesOftrainingdata = feacturesOftrainingdata\n",
        "    self.lablesOftrainingdata = lablesOftrainingdata\n",
        "    self.slidingHead = 0\n",
        "    self.etha = self.parameters['LEARNING_RATE']\n",
        "    self.numberOfLayers = self.parameters['NUMBER_OF_LAYRS']\n",
        "    self.layers =  np.empty(self.numberOfLayers,dtype=Layer)\n",
        "    self.makeLayers()\n",
        "    \n",
        "    \n",
        "\n",
        "  def makeLayers(self):\n",
        "    for i in range(self.numberOfLayers):\n",
        "      self.layers[i] = Layer(i , self)\n",
        "\n",
        "  def train(self):\n",
        "    return self.calculateMLP_outputForRow_k(self.getCurrentFeatureRow())\n",
        "\n",
        "\n",
        "  def calculateMLP_outputForRow_k(self, X):\n",
        "    return self.layers[self.numberOfLayers - 1].claculateLayerOutput(X)\n",
        "\n",
        "  def getCurrentFeatureRow(self):\n",
        "    return self.feacturesOftrainingdata[self.slidingHead]\n",
        "\n",
        "  def getCurrentLableRow(self):\n",
        "    return self.lablesOftrainingdata[self.slidingHead]\n",
        "\n",
        "\n",
        "  def updateAllWeightsByBackPropagationAlgorithm(self):\n",
        "    for layer in self.layers[:0:-1]:\n",
        "      layer.updateLayerWeights( False)\n",
        "    for layer in self.layers[:0:-1]:\n",
        "      layer.updateLayerWeights( True)\n",
        "    self.resetAllcaches()\n",
        "    \n",
        "\n",
        "  \n",
        "  def trainingModel(self, epoch=1):\n",
        "    for i in range(epoch):\n",
        "      self.slidingHead =0\n",
        "      for j in range(self.feacturesOftrainingdata.shape[0]):\n",
        "        self.updateAllWeightsByBackPropagationAlgorithm()\n",
        "        self.slidingHead +=1\n",
        "\n",
        "  def resetAllcaches(self):\n",
        "    for i in range(self.numberOfLayers):\n",
        "      self.layers[i].resetOutput()\n",
        "      for j in range(self.layers[i].numberOfPerceptrons):\n",
        "        self.layers[i].perceptrons[j].resetDelta()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def clearAll(self):\n",
        "      for i in range(self.numberOfLayers):\n",
        "        self.layers[i].resetOutput()\n",
        "        for j in range(self.layers[i].numberOfPerceptrons):\n",
        "          self.layers[i].perceptrons[j].resetDelta()\n",
        "          for k in range(self.layers[i].perceptrons[j].numberOfInputs):\n",
        "            self.layers[i].perceptrons[j].inputBranchs[k].setW()\n",
        "            print(self.layers[i].perceptrons[j].inputBranchs[k].w)\n",
        "          \n",
        "\n",
        "      \n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "#--------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "class Layer:\n",
        "\n",
        "  def __init__(self,layerAddress , MLP):\n",
        "    self.MLP = MLP\n",
        "    self.layerAddress = layerAddress\n",
        "    self.numberOfPerceptrons = self.setNumberOfPerceptrons()\n",
        "    self.activityFunction = ActivityFunction(self)\n",
        "    self.perceptrons =  np.empty(self.numberOfPerceptrons,dtype=Perceptron)\n",
        "    self.perceptrons = self.makePerceptrons()\n",
        "    self.output = np.full((self.numberOfPerceptrons), math.inf)\n",
        "\n",
        "\n",
        "  def resetOutput(self):\n",
        "    self.output = np.full((self.numberOfPerceptrons), math.inf)\n",
        "\n",
        "\n",
        "  def setNumberOfPerceptrons(self):\n",
        "    if(self.layerAddress == 0 ):\n",
        "      return  self.MLP.feacturesOftrainingdata.shape[1]\n",
        "    if(self.layerAddress == self.MLP.numberOfLayers - 1 ):\n",
        "      return  self.MLP.lablesOftrainingdata.shape[1]\n",
        "    return self.MLP.parameters['NUMBER_OF_PERCEPTRONS_FOR_HIDDEN_LAYERS'][self.layerAddress-1]\n",
        "\n",
        "  def makePerceptrons(self):\n",
        "    perceptrons =  np.empty(self.numberOfPerceptrons,dtype=Perceptron) \n",
        "    for i in range( self.numberOfPerceptrons ):\n",
        "      perceptrons[i] = Perceptron( i , self)\n",
        "    return perceptrons\n",
        "\n",
        "  def getPreviosLayer(self):\n",
        "    return self.layerAddress != 0 and self.MLP.layers[self.layerAddress - 1 ] or -1\n",
        "\n",
        "  \n",
        "  def getNextLayer(self):\n",
        "    return self.layerAddress != self.MLP.numberOfLayers - 1 \\\n",
        "     and self.MLP.layers[self.layerAddress + 1 ] or -1\n",
        "\n",
        "\n",
        "  def claculateLayerOutput(self,X):     # receive Vector   # return Vector\n",
        "\n",
        "    if ((any(self.output==math.inf))==False):\n",
        "      return self.output\n",
        "    if(self.layerAddress==0):\n",
        "      X = X\n",
        "      return X\n",
        "    else:\n",
        "      X = self.getPreviosLayer().claculateLayerOutput(X)\n",
        "    output =  np.empty(self.numberOfPerceptrons)\n",
        "    for i in range(self.numberOfPerceptrons):\n",
        "      if(self.layerAddress == 0 ):\n",
        "        output[i] = X[i]\n",
        "      else:\n",
        "        output[i] = self.perceptrons[i].calculatePerceptronOutput(X)\n",
        "    self.output = output\n",
        "    return self.output\n",
        "\n",
        "  def calculateDerivativeOfActivationFunction(self,net):\n",
        "    return self.activityFunction.calculateDerivative(net)\n",
        "\n",
        "  def updateLayerWeights(self, hardUpdate = False):\n",
        "    for perceptron in self.perceptrons:\n",
        "      perceptron.updatePerceptronWeights(hardUpdate)\n",
        "      # if (hardUpdate):\n",
        "        # perceptron.resetDelta()\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "#--------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ActivityFunction:\n",
        "  \n",
        "  def __init__(self,layer):\n",
        "    self.layer = layer\n",
        "    self.functionType = self.layer.MLP.parameters['CODE_OF_ACTIVATION_FUNCTIONS'][self.layer.layerAddress]\n",
        "  \n",
        "  def runActivationFunction(self,x):\n",
        "    if (self.functionType == 1) :\n",
        "      return self.sigmoid(x)\n",
        "    if (self.functionType == 2) :\n",
        "      return self.tanh(x)\n",
        "    if (self.functionType == 3) :\n",
        "      return self.ReLU(x)\n",
        "    if (self.functionType == 4) :\n",
        "      return self.linear(x)\n",
        "\n",
        "  def sigmoid(self, x):\n",
        "    return 1 / (1 + math.exp(-x))\n",
        "\n",
        "  def tanh(self , x):\n",
        "    t=(math.exp(x)-math.exp(-x))/(math.exp(x)+math.exp(-x))\n",
        "    return t\n",
        "\n",
        "  def ReLU(self ,x):\n",
        "    return max(0.0,x)\n",
        "\n",
        "  def linear(self , x):\n",
        "    return x\n",
        "\n",
        "  def calculateDerivative(self , net):\n",
        "    if (self.functionType == 1) :\n",
        "      sig = self.sigmoid(net)\n",
        "      return (1-sig)*sig\n",
        "    if (self.functionType == 2) :\n",
        "      return 1 - self.tanh(net)**2\n",
        "    if (self.functionType == 3) :\n",
        "      if(net<0):\n",
        "        return 0\n",
        "      return 1\n",
        "    if (self.functionType == 4) :\n",
        "      return 1\n",
        "\n",
        "\n",
        "\n",
        "#--------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "class Perceptron:\n",
        "\n",
        "  \n",
        "\n",
        "  def __init__(self , perceptronNumber , layer ):   # [layerAddress  ,  perceptron] \n",
        "    self.perceptronNumber = perceptronNumber\n",
        "    self.layer = layer\n",
        "    self.numberOfInputs  =  self.getNumberOfInputs()\n",
        "    self.inputBranchs =  np.empty(self.numberOfInputs,dtype=Layer)\n",
        "    self.makeInputs()\n",
        "    self.delta = math.inf\n",
        "\n",
        "  def resetDelta(self):\n",
        "    self.delta = math.inf\n",
        "\n",
        "  def makeInputs(self):\n",
        "    for i in range(self.numberOfInputs):\n",
        "      self.inputBranchs[i] = InputBranch(self , i)\n",
        "\n",
        "  def getNumberOfInputs(self):\n",
        "    if(self.layer.layerAddress == 0 ):\n",
        "      return  1\n",
        "    return self.layer.getPreviosLayer().numberOfPerceptrons + 0  #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@  return self.layer.getPreviosLayer().numberOfPerceptrons + 1 \n",
        "\n",
        "  def calculatePerceptronOutput(self , X):\n",
        "        net = self.calculatePerceptronNet(X)\n",
        "        return self.layer.activityFunction.runActivationFunction(net)\n",
        "\n",
        "        \n",
        "  def calculatePerceptronNet(self , X):    # X is input feature vector\n",
        "        y=0\n",
        "        # DONT FORGET BAIAS\n",
        "        X = np.concatenate((X, [0]), axis=None)  #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@   X = np.concatenate((X, [1]), axis=None) \n",
        "        for i in range(self.numberOfInputs):\n",
        "          y += self.inputBranchs[i].calculateBranchOutput(X[i])\n",
        "        return y\n",
        "\n",
        "\n",
        "  def getDelta(self):\n",
        "    if(self.delta != math.inf):\n",
        "      return self.delta\n",
        "    desiredOutput=0\n",
        "    if(self.layer.layerAddress==self.layer.MLP.numberOfLayers - 1):\n",
        "      desiredOutput = self.layer.MLP.getCurrentLableRow()[self.perceptronNumber]\n",
        "    X = (self.layer.layerAddress == 0)  and self.layer.MLP.getCurrentFeatureRow() or self.layer.getPreviosLayer().claculateLayerOutput(self.layer.MLP.getCurrentFeatureRow())\n",
        "    self.delta =  self.calculateDelta(X ,desiredOutput)\n",
        "    return self.delta\n",
        "    # print(self.layer.layerAddress, self.perceptronNumber,self.delta)\n",
        "\n",
        "  def calculateDelta(self,X , desiredOutput):  # X is input vector \n",
        "    net = self.calculatePerceptronNet(X)\n",
        "    if(self.layer.layerAddress == self.layer.MLP.numberOfLayers - 1):     # perceptron in output layer\n",
        "      return self.layer.calculateDerivativeOfActivationFunction(net) * ( desiredOutput - self.calculatePerceptronOutput(X))\n",
        "    else:       # perceptron in hidden layer\n",
        "      sigma = 0\n",
        "      # layerOutput = self.layer.claculateLayerOutput(self.layer.MLP.getCurrentFeatureRow())\n",
        "      for perceptron in self.layer.getNextLayer().perceptrons:\n",
        "        sigma += perceptron.inputBranchs[self.perceptronNumber].w * perceptron.getDelta() \n",
        "      return self.layer.calculateDerivativeOfActivationFunction(net) * sigma\n",
        "\n",
        "  \n",
        "  def updatePerceptronWeights(self,hardUpdate = False):\n",
        "    for inputBranch in self.inputBranchs:\n",
        "      hardUpdate and inputBranch.updateW() or inputBranch.updateWnew()\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "  \n",
        "#--------------------------------------------------------------------------\n",
        "\n",
        "class InputBranch:\n",
        "  \n",
        "  def __init__(self , perceptron, inputNumber):\n",
        "    self.inputNumber = inputNumber\n",
        "    self.perceptron = perceptron\n",
        "    self.setW()\n",
        "    self.Wnew = self.w\n",
        "\n",
        "  def setW(self):\n",
        "    if(self.perceptron.layer.layerAddress == 0):\n",
        "      self.w =  1\n",
        "    self.w = 1# random.uniform(-50, 50) #00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
        "\n",
        "  def calculateBranchOutput(self , x):\n",
        "    return self.w * x \n",
        "\n",
        "  \n",
        "\n",
        "  def updateWnew(self):\n",
        "    self.Wnew =self.w +  self.perceptron.layer.MLP.etha * self.perceptron.getDelta() *\\\n",
        "     np.concatenate((self.perceptron.layer.getPreviosLayer().claculateLayerOutput(self.perceptron.layer.MLP.getCurrentFeatureRow()), [0]), axis=None)[self.inputNumber] #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
        "\n",
        "  def updateW(self):\n",
        "    self.w = self.Wnew\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eAQqhLzIsTfX"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iris = datasets.load_iris()\n",
        "features = iris.data  \n",
        "target = pd.get_dummies(iris.target).to_numpy()\n",
        "features.shape   # (150, 4)\n",
        "target.shape\n",
        "\n",
        "#---------------shuffle---------------------\n",
        "dataset = np.hstack(( features,target,np.reshape(iris.target,(-1,1))))\n",
        "from sklearn.utils import shuffle\n",
        "dataset=shuffle(dataset)\n",
        "\n",
        "#-------------test & train ---------------\n",
        "train=dataset[0:120,:]    \n",
        "test=dataset[120:,:]  \n",
        "test.shape                #(30, 7)\n",
        "train.shape              # (120, 7)\n",
        "\n",
        "\n",
        "# dataset[149]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doXsBMA0wh8K",
        "outputId": "19000bac-2a92-4575-aa20-1e369e8b6367"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(120, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ddd = load_data()\n",
        "s1= ddd['data']\n",
        "s2= ddd['target']\n",
        "s1 = np.array(s1)\n",
        "s2 = np.array(s2)\n",
        "s2.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IOY_fljzX8L",
        "outputId": "9d5848e0-5d93-47b9-9376-4d6b20a2c896"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(200, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PARAMS = {\n",
        "    # enter learning rate :0.1\n",
        "    # enter number of layers :3\n",
        "    # enter code of function for layer0 =>[ 1.sigmoid  | 2.tanh  | 3.relu | 4.linear ] :4\n",
        "    # enter number of Perceptrons for  layer 1 (start layer number from 0) : 2\n",
        "    # enter code of function for layer1 =>[ 1.sigmoid  | 2.tanh  | 3.relu | 4.linear ] :4\n",
        "    # enter code of function for layer2 =>[ 1.sigmoid  | 2.tanh  | 3.relu | 4.linear ] :4\n",
        "    'LEARNING_RATE' : 0.1 ,\n",
        "    'NUMBER_OF_LAYRS' : 3 ,\n",
        "    'CODE_OF_ACTIVATION_FUNCTIONS' : [4,4,4] ,\n",
        "    'NUMBER_OF_PERCEPTRONS_FOR_HIDDEN_LAYERS' : [2]\n",
        " }\n",
        "\n",
        "\n",
        "PARAMS['NUMBER_OF_PERCEPTRONS_FOR_HIDDEN_LAYERS']\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLe2NNZdJk3N",
        "outputId": "c7c0ffab-0fc2-47f7-a8d7-7da9d5699b4e"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2]"
            ]
          },
          "metadata": {},
          "execution_count": 171
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# IRIS_MLP = MLP(train[:,0:4] ,train[:,4:7] )\n",
        "# IRIS_MLP = MLP(train[:,0:4] ,5 * np.reshape(train[:,7],(-1,1,1)) )\n",
        "IRIS_MLP = MLP(np.array([[1,1]]) ,np.array([[2,2]]) , PARAMS )\n",
        "# IRIS_MLP = MLP(s1 ,s2 )"
      ],
      "metadata": {
        "id": "lVlOA3PMs-br"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IRIS_MLP.clearAll()"
      ],
      "metadata": {
        "id": "ytZ-BxlTaRr1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6a45e55-2fe8-4d93-faec-64aa460948ec"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IRIS_MLP.trainingModel(1)"
      ],
      "metadata": {
        "id": "Z7dnEwAciqU8"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s=0\n",
        "# for i in test:\n",
        "  # print([IRIS_MLP.calculateMLP_outputForRow_k(s1[i])[0] ,IRIS_MLP.calculateMLP_outputForRow_k(s1[i])[1]] , list(s2[i]))\n",
        "  # s+=int([int(IRIS_MLP.calculateMLP_outputForRow_k(s1[i])[0]>0) ,int(IRIS_MLP.calculateMLP_outputForRow_k(s1[i])[1]>0)] == list(s2[i]))\n",
        "# \n",
        "\n",
        "  # print(IRIS_MLP.calculateMLP_outputForRow_k(i[0:4]) , i[4:7])\n",
        "\n",
        "\n",
        "print(IRIS_MLP.calculateMLP_outputForRow_k([1,1]) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPZptHsrv_fL",
        "outputId": "d4db5aa0-8ce6-4722-961f-c5f4532cd90c"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.78062336 1.78062336]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(IRIS_MLP.numberOfLayers):\n",
        "  print('layer'+str(i))\n",
        "  for j in range(IRIS_MLP.layers[i].numberOfPerceptrons):\n",
        "    print('percepron'+str(j))\n",
        "    for k in range(IRIS_MLP.layers[i].perceptrons[j].numberOfInputs):\n",
        "      print(IRIS_MLP.layers[i].perceptrons[j].inputBranchs[k].w)"
      ],
      "metadata": {
        "id": "LqH9EhAf5w1y",
        "outputId": "3c6acb7c-c923-4504-a3a7-15ce00d2d879",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "layer0\n",
            "percepron0\n",
            "1\n",
            "percepron1\n",
            "1\n",
            "layer1\n",
            "percepron0\n",
            "0.6672\n",
            "0.6672\n",
            "percepron1\n",
            "0.6672\n",
            "0.6672\n",
            "layer2\n",
            "percepron0\n",
            "0.6672\n",
            "0.6672\n",
            "percepron1\n",
            "0.6672\n",
            "0.6672\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(0.6 - 1.12*2.4*0.1)*1.12*2"
      ],
      "metadata": {
        "id": "VE9CRdK-7LJV",
        "outputId": "89934568-be1c-4440-8537-835c3597b9d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7418879999999999"
            ]
          },
          "metadata": {},
          "execution_count": 254
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(0.6-0.1*0.7418*2)*2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pa2e9cmBuV5d",
        "outputId": "59680e5b-d091-4dec-dd47-6e1cddd8cfc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9032799999999999"
            ]
          },
          "metadata": {},
          "execution_count": 256
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "0.9032799999999999*0.3311*2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZYeb6FpvwQr",
        "outputId": "370fb5c3-5e54-4065-cd87-d715a61eaf1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5981520159999999"
            ]
          },
          "metadata": {},
          "execution_count": 257
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "uh92iRq4weQE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}