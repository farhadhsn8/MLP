{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "implementationDynamicMLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMfB7Ex/Pg9EgxoOkUxKFdW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/farhadhsn8/MLP/blob/master/implementationDynamicMLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attribute Information:\n",
        "\n",
        "1. sepal length in cm\n",
        "2. sepal width in cm\n",
        "3. petal length in cm\n",
        "4. petal width in cm\n",
        "\n",
        "## class:\n",
        "1. Iris Setosa\n",
        "2. Iris Versicolour\n",
        "3. Iris Virginica"
      ],
      "metadata": {
        "id": "oe3sbQBguksr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 255,
      "metadata": {
        "id": "Iky3dAh7qIXZ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "import random\n",
        "import numpy as np\n",
        "import math\n",
        "iris = datasets.load_iris()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iris = datasets.load_iris()\n",
        "features = iris.data  \n",
        "lables = iris.target\n",
        "type(features)\n",
        "features.shape   # (150, 4)\n",
        "lables.shape    # (150,)\n",
        "NUMBER_OF_OUTPUT = 2"
      ],
      "metadata": {
        "id": "0hG14l9kq5pd"
      },
      "execution_count": 266,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xx = np.array([1,2,3,4])\n",
        "xx[:0:-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHYZsQ0tN3-4",
        "outputId": "912df895-13e9-472e-d19c-0fa53c0cab54"
      },
      "execution_count": 298,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4, 3, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 298
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP:\n",
        "\n",
        "  def __init__(self , feacturesOftrainingdata , lablesOftrainingdata):\n",
        "    self.feacturesOftrainingdata = feacturesOftrainingdata\n",
        "    self.lablesOftrainingdata = lablesOftrainingdata\n",
        "    self.slidingHead = 0\n",
        "    self.etha = float(input('enter learning rate :'))\n",
        "    self.numberOfLayers = int(input('enter number of layers :'))\n",
        "    self.layers =  np.empty(self.numberOfLayers,dtype=Layer)\n",
        "    self.makeLayers()\n",
        "    \n",
        "    \n",
        "\n",
        "  def makeLayers(self):\n",
        "    for i in range(self.numberOfLayers):\n",
        "      self.layers[i] = Layer(i , self)\n",
        "\n",
        "  def train(self):\n",
        "    return self.calculateMLP_outputForRow_k(self.getCurrentFeacherRow())\n",
        "\n",
        "\n",
        "  def calculateMLP_outputForRow_k(self, X):\n",
        "    return self.layers[self.numberOfLayers - 1].claculateLayerOutput()\n",
        "\n",
        "  def getCurrentFeacherRow(self):\n",
        "    return self.feacturesOftrainingdata[self.slidingHead]\n",
        "\n",
        "  def getCurrentLableRow(self):\n",
        "    return self.lablesOftrainingdata[self.slidingHead]\n",
        "\n",
        "\n",
        "  def updateAllWeightsByBackPropagationAlgorithm(self):\n",
        "    for layer in self.layers[:0:-1]:\n",
        "      layer.updateLayerWeights( False)\n",
        "    for layer in self.layers[:0:-1]:\n",
        "      layer.updateLayerWeights( True)\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "#--------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "class Layer:\n",
        "\n",
        "  def __init__(self,layerAddress , MLP):\n",
        "    self.MLP = MLP\n",
        "    self.layerAddress = layerAddress\n",
        "    self.numberOfPerceptrons = self.setNumberOfPerceptrons()\n",
        "    self.activityFunction = ActivityFunction(self)\n",
        "    self.perceptrons =  np.empty(self.numberOfPerceptrons,dtype=Perceptron)\n",
        "    self.perceptrons = self.makePerceptrons()\n",
        "    self.output = np.full((self.numberOfPerceptrons), math.inf)\n",
        "\n",
        "\n",
        "  def resetOutput(self):\n",
        "    self.output = np.full((self.numberOfPerceptrons), math.inf)\n",
        "\n",
        "\n",
        "  def setNumberOfPerceptrons(self):\n",
        "    if(self.layerAddress == 0 ):\n",
        "      return  2 #features.shape[1] + 1  ###############\n",
        "    if(self.layerAddress == self.MLP.numberOfLayers - 1 ):\n",
        "      return  NUMBER_OF_OUTPUT\n",
        "    return int(input('enter number of Perceptrons for  layer '+ str(self.layerAddress  ) + ' (start layer number from 0) : '))\n",
        "\n",
        "  def makePerceptrons(self):\n",
        "    perceptrons =  np.empty(self.numberOfPerceptrons,dtype=Perceptron) \n",
        "    for i in range( self.numberOfPerceptrons ):\n",
        "      perceptrons[i] = Perceptron( i , self)\n",
        "    return perceptrons\n",
        "\n",
        "  def getPreviosLayer(self):\n",
        "    return self.layerAddress != 0 and self.MLP.layers[self.layerAddress - 1 ] or -1\n",
        "\n",
        "  \n",
        "  def getNextLayer(self):\n",
        "    return self.layerAddress != self.MLP.numberOfLayers - 1 \\\n",
        "     and self.MLP.layers[self.layerAddress + 1 ] or -1\n",
        "\n",
        "\n",
        "  def claculateLayerOutput(self):     # receive Vector   # return Vector\n",
        "    if (self.output[0] !=   math.inf):\n",
        "      return self.output\n",
        "    X = self.layerAddress==0 and self.MLP.getCurrentFeacherRow() or self.getPreviosLayer().claculateLayerOutput()\n",
        "    output =  np.empty(self.numberOfPerceptrons)\n",
        "    for i in range(self.numberOfPerceptrons):\n",
        "      output[i] =(self.layerAddress == 0) and self.perceptrons[i].calculatePerceptronOutput([X[i]]) \\\n",
        "       or self.perceptrons[i].calculatePerceptronOutput(X)\n",
        "    self.output = output\n",
        "    return output\n",
        "\n",
        "  def calculateDerivativeOfActivationFunction(self,net):\n",
        "    return self.activityFunction.calculateDerivative(net)\n",
        "\n",
        "  def updateLayerWeights(self, hardUpdate = False):\n",
        "    for perceptron in self.perceptrons:\n",
        "      perceptron.updatePerceptronWeights(hardUpdate)\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "#--------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ActivityFunction:\n",
        "  \n",
        "  def __init__(self,layer):\n",
        "    self.layer = layer\n",
        "    self.functionType = int(input('enter code of function for layer'+str(self.layer.layerAddress)+' =>[ 1.sigmoid  | 2.tanh  | 3.relu | 4.linear ] :'))\n",
        "  \n",
        "  def runActivationFunction(self,x):\n",
        "    if (self.functionType == 1) :\n",
        "      return self.sigmoid(x)\n",
        "    if (self.functionType == 2) :\n",
        "      return self.tanh(x)\n",
        "    if (self.functionType == 3) :\n",
        "      return self.ReLU(x)\n",
        "    if (self.functionType == 4) :\n",
        "      return self.linear(x)\n",
        "\n",
        "  def sigmoid(self, x):\n",
        "    return 1 / (1 + math.exp(-x))\n",
        "\n",
        "  def tanh(self , x):\n",
        "    t=(math.exp(x)-math.exp(-x))/(math.exp(x)+math.exp(-x))\n",
        "    return t\n",
        "\n",
        "  def ReLU(self ,x):\n",
        "    return max(0.0,x)\n",
        "\n",
        "  def linear(self , x):\n",
        "    return x\n",
        "\n",
        "  def calculateDerivative(self , net):\n",
        "    if (self.functionType == 1) :\n",
        "      sig = self.sigmoid(net)\n",
        "      return (1-sig)*sig\n",
        "    if (self.functionType == 2) :\n",
        "      return 1 - self.tanh(net)**2\n",
        "    if (self.functionType == 3) :\n",
        "      if(net<0):\n",
        "        return 0\n",
        "      return 1\n",
        "    if (self.functionType == 4) :\n",
        "      return 1\n",
        "\n",
        "\n",
        "\n",
        "#--------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "class Perceptron:\n",
        "\n",
        "  \n",
        "\n",
        "  def __init__(self , perceptronNumber , layer ):   # [layerAddress  ,  perceptron] \n",
        "    self.perceptronNumber = perceptronNumber\n",
        "    self.layer = layer\n",
        "    self.numberOfInputs  =  self.getNumberOfInputs()\n",
        "    self.inputBranchs =  np.empty(self.numberOfInputs,dtype=Layer)\n",
        "    self.makeInputs()\n",
        "    self.delta = math.inf\n",
        "\n",
        "  def resetDelta(self):\n",
        "    self.delta = math.inf\n",
        "\n",
        "  def makeInputs(self):\n",
        "    for i in range(self.numberOfInputs):\n",
        "      self.inputBranchs[i] = InputBranch(self , i)\n",
        "\n",
        "  def getNumberOfInputs(self):\n",
        "    if(self.layer.layerAddress == 0 ):\n",
        "      return  1\n",
        "    return self.layer.getPreviosLayer().numberOfPerceptrons + 1\n",
        "\n",
        "  def calculatePerceptronOutput(self , X):\n",
        "        net = self.calculatePerceptronNet(X)\n",
        "        return self.layer.activityFunction.runActivationFunction(net)\n",
        "\n",
        "        \n",
        "  def calculatePerceptronNet(self , X):    # X is input feature vector\n",
        "        y=0\n",
        "        # DONT FORGET BAIAS\n",
        "        X = np.concatenate((X, [1]), axis=None)\n",
        "        for i in range(self.numberOfInputs):\n",
        "          y += self.inputBranchs[i].calculateBranchOutput(X[i])\n",
        "        return y\n",
        "\n",
        "\n",
        "  def getDelta(self):\n",
        "    desiredOutput = self.layer.MLP.getCurrentLableRow()[self.perceptronNumber]\n",
        "    X = (self.layer.layerAddress == 0)  and self.layer.MLP.getCurrentFeacherRow() or self.layer.getPreviosLayer().claculateLayerOutput()\n",
        "    if(self.delta == math.inf):\n",
        "      self.delta =  self.calculateDelta(X ,desiredOutput)\n",
        "      return self.calculateDelta(X ,desiredOutput)\n",
        "    return self.delta\n",
        "\n",
        "  def calculateDelta(self,X , desiredOutput):  # X is input vector \n",
        "    net = self.calculatePerceptronNet(X)\n",
        "    if(self.layer.layerAddress == self.layer.MLP.numberOfLayers - 1):     # perceptron in output layer\n",
        "      return self.layer.calculateDerivativeOfActivationFunction(net) * ( desiredOutput - self.calculatePerceptronOutput(X))\n",
        "    else:       # perceptron in hidden layer\n",
        "      sigma = 0\n",
        "      layerOutput = self.layer.claculateLayerOutput()\n",
        "      for perceptron in self.layer.getNextLayer().perceptrons:\n",
        "        sigma += perceptron.inputBranchs[self.perceptronNumber].w * perceptron.getDelta() \n",
        "      return self.layer.calculateDerivativeOfActivationFunction(net) * sigma\n",
        "\n",
        "  \n",
        "  def updatePerceptronWeights(self,hardUpdate = False):\n",
        "    print('perceptron : ' , self.perceptronNumber)\n",
        "    for inputBranch in self.inputBranchs:\n",
        "      print('branch : ', inputBranch.inputNumber)\n",
        "      hardUpdate and inputBranch.updateW() or inputBranch.updateWnew()\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "  \n",
        "          \n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#--------------------------------------------------------------------------\n",
        "\n",
        "class InputBranch:\n",
        "  \n",
        "  def __init__(self , perceptron, inputNumber):\n",
        "    self.inputNumber = inputNumber\n",
        "    self.perceptron = perceptron\n",
        "    self.w = self.setW()\n",
        "    self.Wnew = self.w\n",
        "\n",
        "  def setW(self):\n",
        "    if(self.perceptron.layer.layerAddress == 0):\n",
        "      return 1\n",
        "    return 1#random.uniform(-3, 3)\n",
        "\n",
        "  def calculateBranchOutput(self , x):\n",
        "    return self.w * x \n",
        "\n",
        "  def updateWnew(self):\n",
        "    print('inp = ' , self.inputNumber)\n",
        "    self.Wnew =self.w +  self.perceptron.layer.MLP.etha * self.perceptron.getDelta() *\\\n",
        "     np.concatenate((self.perceptron.layer.getPreviosLayer().claculateLayerOutput(), [1]), axis=None)[self.inputNumber]\n",
        "\n",
        "  def updateW(self):\n",
        "    self.w = self.Wnew\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eAQqhLzIsTfX"
      },
      "execution_count": 299,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A = MLP([[1,2,3,]] , [[0,1,1]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doXsBMA0wh8K",
        "outputId": "8819f360-b26b-4639-8101-db952c0989b7"
      },
      "execution_count": 300,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "enter learning rate :0.1\n",
            "enter number of layers :3\n",
            "enter code of function for layer0 =>[ 1.sigmoid  | 2.tanh  | 3.relu | 4.linear ] :4\n",
            "enter number of Perceptrons for  layer 1 (start layer number from 0) : 2\n",
            "enter code of function for layer1 =>[ 1.sigmoid  | 2.tanh  | 3.relu | 4.linear ] :4\n",
            "enter code of function for layer2 =>[ 1.sigmoid  | 2.tanh  | 3.relu | 4.linear ] :4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# shape of MLP\n",
        "for i in A.layers:\n",
        "  print(i)\n",
        "  for j in i.perceptrons:\n",
        "    print( j.numberOfInputs)"
      ],
      "metadata": {
        "id": "lVlOA3PMs-br",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f23c286-2d1d-4987-e85d-afbb7f690b46"
      },
      "execution_count": 302,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<__main__.Layer object at 0x7f6e6c149b90>\n",
            "1\n",
            "1\n",
            "<__main__.Layer object at 0x7f6e6b4ed490>\n",
            "3\n",
            "3\n",
            "<__main__.Layer object at 0x7f6e6b5456d0>\n",
            "3\n",
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "A.train()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zx3cI2YuhIQ-",
        "outputId": "67470603-2bcc-4a6c-c81c-9e817d989c98"
      },
      "execution_count": 303,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([9., 9.])"
            ]
          },
          "metadata": {},
          "execution_count": 303
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(A.numberOfLayers):\n",
        "  print('layer'+str(i))\n",
        "  for j in range(A.layers[i].numberOfPerceptrons):\n",
        "    print('percepron'+str(j))\n",
        "    for k in range(A.layers[i].perceptrons[j].numberOfInputs):\n",
        "      print(A.layers[i].perceptrons[j].inputBranchs[k].w)\n"
      ],
      "metadata": {
        "id": "BzVIsbnZzUPT",
        "outputId": "97049589-c38b-4093-c360-8b5544ecd47d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 304,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "layer0\n",
            "percepron0\n",
            "1\n",
            "percepron1\n",
            "1\n",
            "layer1\n",
            "percepron0\n",
            "1\n",
            "1\n",
            "1\n",
            "percepron1\n",
            "1\n",
            "1\n",
            "1\n",
            "layer2\n",
            "percepron0\n",
            "1\n",
            "1\n",
            "1\n",
            "percepron1\n",
            "1\n",
            "1\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A.layers[0].perceptrons[0].getDelta()\n"
      ],
      "metadata": {
        "id": "mGXPZTAR9aTq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc661e88-8606-4af9-a979-e79f48b8bced"
      },
      "execution_count": 305,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-34.0"
            ]
          },
          "metadata": {},
          "execution_count": 305
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A.layers[1].perceptrons[1].inputBranchs[0].updateW()\n"
      ],
      "metadata": {
        "id": "DGgkOyR7M9rf"
      },
      "execution_count": 264,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A.updateAllWeightsByBackPropagationAlgorithm()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3iq57kTlCpG",
        "outputId": "77b1ace7-2483-400a-cafd-6910bb430bf3"
      },
      "execution_count": 306,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "perceptron :  0\n",
            "branch :  0\n",
            "inp =  0\n",
            "branch :  1\n",
            "inp =  1\n",
            "branch :  2\n",
            "inp =  2\n",
            "perceptron :  1\n",
            "branch :  0\n",
            "inp =  0\n",
            "branch :  1\n",
            "inp =  1\n",
            "branch :  2\n",
            "inp =  2\n",
            "perceptron :  0\n",
            "branch :  0\n",
            "inp =  0\n",
            "branch :  1\n",
            "inp =  1\n",
            "branch :  2\n",
            "inp =  2\n",
            "perceptron :  1\n",
            "branch :  0\n",
            "inp =  0\n",
            "branch :  1\n",
            "inp =  1\n",
            "branch :  2\n",
            "inp =  2\n",
            "perceptron :  0\n",
            "branch :  0\n",
            "inp =  0\n",
            "branch :  1\n",
            "inp =  1\n",
            "branch :  2\n",
            "inp =  2\n",
            "perceptron :  1\n",
            "branch :  0\n",
            "inp =  0\n",
            "branch :  1\n",
            "inp =  1\n",
            "branch :  2\n",
            "inp =  2\n",
            "perceptron :  0\n",
            "branch :  0\n",
            "inp =  0\n",
            "branch :  1\n",
            "inp =  1\n",
            "branch :  2\n",
            "inp =  2\n",
            "perceptron :  1\n",
            "branch :  0\n",
            "inp =  0\n",
            "branch :  1\n",
            "inp =  1\n",
            "branch :  2\n",
            "inp =  2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for l in A.layers:\n",
        "  print('Layer ', l.layerAddress)\n",
        "  for p in l.perceptrons:\n",
        "    print('Perceptron ', p.perceptronNumber)\n",
        "    for i in p.inputBranchs:\n",
        "      print(i.w)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dnrgm7uJ8yfZ",
        "outputId": "f8b7f719-b493-45d8-84b2-5e3ae3635d04"
      },
      "execution_count": 307,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer  0\n",
            "Perceptron  0\n",
            "1\n",
            "Perceptron  1\n",
            "1\n",
            "Layer  1\n",
            "Perceptron  0\n",
            "-0.7000000000000002\n",
            "-2.4000000000000004\n",
            "-0.7000000000000002\n",
            "Perceptron  1\n",
            "-0.7000000000000002\n",
            "-2.4000000000000004\n",
            "-0.7000000000000002\n",
            "Layer  2\n",
            "Perceptron  0\n",
            "-2.6\n",
            "-2.6\n",
            "0.09999999999999998\n",
            "Perceptron  1\n",
            "-2.2\n",
            "-2.2\n",
            "0.19999999999999996\n"
          ]
        }
      ]
    }
  ]
}