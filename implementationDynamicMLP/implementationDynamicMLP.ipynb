{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/farhadhsn8/MLP/blob/master/implementationDynamicMLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oe3sbQBguksr"
      },
      "source": [
        "## Attribute Information:\n",
        "\n",
        "1. sepal length in cm\n",
        "2. sepal width in cm\n",
        "3. petal length in cm\n",
        "4. petal width in cm\n",
        "\n",
        "## class:\n",
        "1. Iris Setosa\n",
        "2. Iris Versicolour\n",
        "3. Iris Virginica"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Iky3dAh7qIXZ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "import random\n",
        "import numpy as np\n",
        "import math\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "metadata": {
        "id": "eAQqhLzIsTfX"
      },
      "outputs": [],
      "source": [
        "class MLP:\n",
        "\n",
        "#   PARAMS =  {\n",
        "#     # enter learning rate :0.1\n",
        "#     # enter code of function for layer0 =>[ 1.sigmoid  | 2.tanh  | 3.relu | 4.linear ] :4\n",
        "#     # enter number of Perceptrons for  layer 1 (start layer number from 0) : 2\n",
        "#     # enter code of function for layer1 =>[ 1.sigmoid  | 2.tanh  | 3.relu | 4.linear ] :4\n",
        "#     # enter code of function for layer2 =>[ 1.sigmoid  | 2.tanh  | 3.relu | 4.linear ] :4\n",
        "\n",
        "#     'LEARNING_RATE' : 0.1 ,\n",
        "#     'CODE_OF_ACTIVATION_FUNCTIONS' : [4,4,4] ,\n",
        "#     'NUMBER_OF_PERCEPTRONS_FOR_HIDDEN_LAYERS' : [2]\n",
        "#  }\n",
        "\n",
        "  def __init__(self , feacturesOftrainingdata , lablesOftrainingdata , parameters):\n",
        "    self.parameters = parameters\n",
        "    self.feacturesOftrainingdata = feacturesOftrainingdata\n",
        "    self.lablesOftrainingdata = lablesOftrainingdata\n",
        "    self.slidingHead = 0\n",
        "    self.etha = self.parameters['LEARNING_RATE']\n",
        "    self.numberOfLayers = len(self.parameters['CODE_OF_ACTIVATION_FUNCTIONS'])\n",
        "    self.layers =  np.empty(self.numberOfLayers,dtype=Layer)\n",
        "    self.makeLayers()\n",
        "    \n",
        "    \n",
        "\n",
        "  def makeLayers(self):\n",
        "    for i in range(self.numberOfLayers):\n",
        "      self.layers[i] = Layer(i , self)\n",
        "\n",
        "  def train(self):\n",
        "    return self.calculateMLP_outputForRow_k(self.getCurrentFeatureRow())\n",
        "\n",
        "\n",
        "  def calculateMLP_outputForRow_k(self, X):\n",
        "    self.resetAllcaches()\n",
        "    return self.layers[self.numberOfLayers - 1].claculateLayerOutput(X)\n",
        "\n",
        "\n",
        "  def getCurrentFeatureRow(self):\n",
        "    return self.feacturesOftrainingdata[self.slidingHead]\n",
        "\n",
        "  def getCurrentLableRow(self):\n",
        "    return self.lablesOftrainingdata[self.slidingHead]\n",
        "\n",
        "\n",
        "  def updateAllWeightsByBackPropagationAlgorithm(self):\n",
        "    for layer in self.layers[:0:-1]:\n",
        "      layer.updateLayerWeights( False)\n",
        "    for layer in self.layers[:0:-1]:\n",
        "      layer.updateLayerWeights( True)\n",
        "    self.resetAllcaches()\n",
        "    \n",
        "\n",
        "  \n",
        "  def trainingModel(self, epoch=1):\n",
        "    \n",
        "    for i in range(epoch):\n",
        "      # printProgressBar(i, epoch, prefix = 'Progress:', suffix = 'Complete', length = 50)\n",
        "      self.slidingHead =0\n",
        "      for j in range(self.feacturesOftrainingdata.shape[0]):\n",
        "        self.updateAllWeightsByBackPropagationAlgorithm()\n",
        "        self.slidingHead +=1\n",
        "      # printProgressBar(i + 1, epoch, prefix = 'Progress:', suffix = 'Complete', length = 50)\n",
        "\n",
        "  def resetAllcaches(self):\n",
        "    for i in range(self.numberOfLayers):\n",
        "      self.layers[i].resetOutput()\n",
        "      for j in range(self.layers[i].numberOfPerceptrons):\n",
        "        self.layers[i].perceptrons[j].resetDelta()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def clearAll(self):\n",
        "      for i in range(self.numberOfLayers):\n",
        "        self.layers[i].resetOutput()\n",
        "        for j in range(self.layers[i].numberOfPerceptrons):\n",
        "          self.layers[i].perceptrons[j].resetDelta()\n",
        "          for k in range(self.layers[i].perceptrons[j].numberOfInputs):\n",
        "            self.layers[i].perceptrons[j].inputBranchs[k].setW()\n",
        "            print(self.layers[i].perceptrons[j].inputBranchs[k].w)\n",
        "          \n",
        "\n",
        "      \n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "#--------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "class Layer:\n",
        "\n",
        "  def __init__(self,layerAddress , MLP):\n",
        "    self.MLP = MLP\n",
        "    self.layerAddress = layerAddress\n",
        "    self.numberOfPerceptrons = self.setNumberOfPerceptrons()\n",
        "    self.activityFunction = ActivityFunction(self)\n",
        "    self.perceptrons =  np.empty(self.numberOfPerceptrons,dtype=Perceptron)\n",
        "    self.perceptrons = self.makePerceptrons()\n",
        "    self.output = np.full((self.numberOfPerceptrons), math.inf)\n",
        "\n",
        "\n",
        "  def resetOutput(self):\n",
        "    self.output = np.full((self.numberOfPerceptrons), math.inf)\n",
        "\n",
        "\n",
        "  def setNumberOfPerceptrons(self):\n",
        "    if(self.layerAddress == 0 ):\n",
        "      return  self.MLP.feacturesOftrainingdata.shape[1]\n",
        "    if(self.layerAddress == self.MLP.numberOfLayers - 1 ):\n",
        "      return  self.MLP.lablesOftrainingdata.shape[1]\n",
        "    return self.MLP.parameters['NUMBER_OF_PERCEPTRONS_FOR_HIDDEN_LAYERS'][self.layerAddress-1]\n",
        "\n",
        "  def makePerceptrons(self):\n",
        "    perceptrons =  np.empty(self.numberOfPerceptrons,dtype=Perceptron) \n",
        "    for i in range( self.numberOfPerceptrons ):\n",
        "      perceptrons[i] = Perceptron( i , self)\n",
        "    return perceptrons\n",
        "\n",
        "  def getPreviosLayer(self):\n",
        "    return self.layerAddress != 0 and self.MLP.layers[self.layerAddress - 1 ] or -1\n",
        "\n",
        "  \n",
        "  def getNextLayer(self):\n",
        "    return self.layerAddress != self.MLP.numberOfLayers - 1 \\\n",
        "     and self.MLP.layers[self.layerAddress + 1 ] or -1\n",
        "\n",
        "\n",
        "  def claculateLayerOutput(self,X):     # receive Vector   # return Vector\n",
        "\n",
        "    if ((any(self.output==math.inf))==False):\n",
        "      return self.output\n",
        "    if(self.layerAddress==0):\n",
        "      X = X\n",
        "      return X\n",
        "    else:\n",
        "      X = self.getPreviosLayer().claculateLayerOutput(X)\n",
        "    output =  np.empty(self.numberOfPerceptrons)\n",
        "    for i in range(self.numberOfPerceptrons):\n",
        "      if(self.layerAddress == 0 ):\n",
        "        output[i] = X[i]\n",
        "      else:\n",
        "        output[i] = self.perceptrons[i].calculatePerceptronOutput(X)\n",
        "    self.output = output\n",
        "    return self.output\n",
        "\n",
        "  def calculateDerivativeOfActivationFunction(self,net):\n",
        "    return self.activityFunction.calculateDerivative(net)\n",
        "\n",
        "  def updateLayerWeights(self, hardUpdate = False):\n",
        "    for perceptron in self.perceptrons:\n",
        "      perceptron.updatePerceptronWeights(hardUpdate)\n",
        "    \n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "#--------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ActivityFunction:\n",
        "  \n",
        "  def __init__(self,layer):\n",
        "    self.layer = layer\n",
        "    self.functionType = self.layer.MLP.parameters['CODE_OF_ACTIVATION_FUNCTIONS'][self.layer.layerAddress]\n",
        "  \n",
        "  def runActivationFunction(self,x):\n",
        "    if (self.functionType == 1) :\n",
        "      return self.sigmoid(x)\n",
        "    if (self.functionType == 2) :\n",
        "      return self.tanh(x)\n",
        "    if (self.functionType == 3) :\n",
        "      return self.ReLU(x)\n",
        "    if (self.functionType == 4) :\n",
        "      return self.linear(x)\n",
        "\n",
        "  def sigmoid(self, x):\n",
        "    return 1 / (1 + math.exp(-x))\n",
        "\n",
        "  def tanh(self , x):\n",
        "    t=(math.exp(x)-math.exp(-x))/(math.exp(x)+math.exp(-x))\n",
        "    return t\n",
        "\n",
        "  def ReLU(self ,x):\n",
        "    return max(0.0,x)\n",
        "\n",
        "  def linear(self , x):\n",
        "    return x\n",
        "\n",
        "  def calculateDerivative(self , net):\n",
        "    if (self.functionType == 1) :\n",
        "      sig = self.sigmoid(net)\n",
        "      return (1-sig)*sig\n",
        "    if (self.functionType == 2) :\n",
        "      return 1 - self.tanh(net)**2\n",
        "    if (self.functionType == 3) :\n",
        "      if(net<0):\n",
        "        return 0\n",
        "      return 1\n",
        "    if (self.functionType == 4) :\n",
        "      return 1\n",
        "\n",
        "\n",
        "\n",
        "#--------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "class Perceptron:\n",
        "\n",
        "  \n",
        "\n",
        "  def __init__(self , perceptronNumber , layer ):   # [layerAddress  ,  perceptron] \n",
        "    self.baias = 0  # 0 or 1\n",
        "    self.perceptronNumber = perceptronNumber\n",
        "    self.layer = layer\n",
        "    self.numberOfInputs  =  self.getNumberOfInputs()\n",
        "    self.inputBranchs =  np.empty(self.numberOfInputs,dtype=Layer)\n",
        "    self.makeInputs()\n",
        "    self.delta = math.inf\n",
        "\n",
        "  def resetDelta(self):\n",
        "    self.delta = math.inf\n",
        "\n",
        "  def makeInputs(self):\n",
        "    for i in range(self.numberOfInputs):\n",
        "      self.inputBranchs[i] = InputBranch(self , i)\n",
        "\n",
        "  def getNumberOfInputs(self):\n",
        "    if(self.layer.layerAddress == 0 ):\n",
        "      return  1\n",
        "    return self.layer.getPreviosLayer().numberOfPerceptrons + self.baias #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@  return self.layer.getPreviosLayer().numberOfPerceptrons + 1 \n",
        "\n",
        "  def calculatePerceptronOutput(self , X):\n",
        "        net = self.calculatePerceptronNet(X)\n",
        "        return self.layer.activityFunction.runActivationFunction(net)\n",
        "\n",
        "        \n",
        "  def calculatePerceptronNet(self , X):    # X is input feature vector\n",
        "        y=0\n",
        "        # DONT FORGET BAIAS\n",
        "        X = np.concatenate((X, [self.baias]), axis=None)  #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@   X = np.concatenate((X, [1]), axis=None) \n",
        "        for i in range(self.numberOfInputs):\n",
        "          y += self.inputBranchs[i].calculateBranchOutput(X[i])\n",
        "        return y\n",
        "\n",
        "\n",
        "  def getDelta(self):\n",
        "    if(self.delta != math.inf):\n",
        "      return self.delta\n",
        "    desiredOutput=0\n",
        "    if(self.layer.layerAddress==self.layer.MLP.numberOfLayers - 1):\n",
        "      desiredOutput = self.layer.MLP.getCurrentLableRow()[self.perceptronNumber]\n",
        "    X = (self.layer.layerAddress == 0)  and self.layer.MLP.getCurrentFeatureRow() or self.layer.getPreviosLayer().claculateLayerOutput(self.layer.MLP.getCurrentFeatureRow())\n",
        "    self.delta =  self.calculateDelta(X ,desiredOutput)\n",
        "    return self.delta\n",
        "    # print(self.layer.layerAddress, self.perceptronNumber,self.delta)\n",
        "\n",
        "  def calculateDelta(self,X , desiredOutput):  # X is input vector \n",
        "    net = self.calculatePerceptronNet(X)\n",
        "    if(self.layer.layerAddress == self.layer.MLP.numberOfLayers - 1):     # perceptron in output layer\n",
        "      return self.layer.calculateDerivativeOfActivationFunction(net) * ( desiredOutput - self.calculatePerceptronOutput(X))\n",
        "    else:       # perceptron in hidden layer\n",
        "      sigma = 0\n",
        "      # layerOutput = self.layer.claculateLayerOutput(self.layer.MLP.getCurrentFeatureRow())\n",
        "      for perceptron in self.layer.getNextLayer().perceptrons:\n",
        "        sigma += (perceptron.inputBranchs[self.perceptronNumber].w * perceptron.getDelta()) \n",
        "      return self.layer.calculateDerivativeOfActivationFunction(net) * sigma\n",
        "\n",
        "  \n",
        "  def updatePerceptronWeights(self,hardUpdate = False):\n",
        "    for inputBranch in self.inputBranchs:\n",
        "      hardUpdate and inputBranch.updateW() or inputBranch.updateWnew()\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "  \n",
        "#--------------------------------------------------------------------------\n",
        "\n",
        "class InputBranch:\n",
        "  \n",
        "  def __init__(self , perceptron, inputNumber):\n",
        "    self.inputNumber = inputNumber\n",
        "    self.perceptron = perceptron\n",
        "    self.setW()\n",
        "    self.Wnew = self.w\n",
        "\n",
        "  def setW(self):\n",
        "    if(self.perceptron.layer.layerAddress == 0):\n",
        "      self.w =  1\n",
        "    self.w = random.uniform(0,1) #00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
        "\n",
        "  def calculateBranchOutput(self , x):\n",
        "    return self.w * x \n",
        "\n",
        "  \n",
        "\n",
        "  def updateWnew(self):\n",
        "    etha = self.perceptron.layer.MLP.etha\n",
        "    yi = np.concatenate((self.perceptron.layer.getPreviosLayer().claculateLayerOutput(self.perceptron.layer.MLP.getCurrentFeatureRow()), [self.perceptron.baias]), axis=None)[self.inputNumber]  #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
        "    self.Wnew =self.w +  etha * self.perceptron.getDelta() * yi \n",
        "\n",
        "  def updateW(self):\n",
        "    self.w = self.Wnew\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 671,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doXsBMA0wh8K",
        "outputId": "475df412-8433-4f2b-cf44-17720f675591"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(120, 8)"
            ]
          },
          "execution_count": 671,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "iris = datasets.load_iris()\n",
        "features = iris.data  \n",
        "target = pd.get_dummies(iris.target).to_numpy()\n",
        "features.shape   # (150, 4)\n",
        "target.shape\n",
        "\n",
        "dataset = np.hstack(( features,target,np.reshape(iris.target,(-1,1))))\n",
        "#---------------shuffle---------------------\n",
        "from sklearn.utils import shuffle\n",
        "dataset=shuffle(dataset)\n",
        "\n",
        "#-------------test & train ---------------\n",
        "train=dataset[0:120,:]    \n",
        "test=dataset[120:,:]  \n",
        "test.shape                #(30, 7)\n",
        "train.shape              # (120, 7)\n",
        "\n",
        "\n",
        "# dataset[149]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 678,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLe2NNZdJk3N",
        "outputId": "9d2df3b2-45e6-4241-c51e-e2562555e043"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[5]"
            ]
          },
          "execution_count": 678,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "PARAMS = {\n",
        "    # enter learning rate :0.1\n",
        "    # enter code of function for layer0 =>[ 1.sigmoid  | 2.tanh  | 3.relu | 4.linear ] :4\n",
        "    # enter number of Perceptrons for  layer 1 (start layer number from 0) : 2\n",
        "    # enter code of function for layer1 =>[ 1.sigmoid  | 2.tanh  | 3.relu | 4.linear ] :4\n",
        "    # enter code of function for layer2 =>[ 1.sigmoid  | 2.tanh  | 3.relu | 4.linear ] :4\n",
        "    'LEARNING_RATE' : 0.01 ,\n",
        "    'CODE_OF_ACTIVATION_FUNCTIONS' : [4,2,3] , #[ 1.sigmoid  | 2.tanh  | 3.relu | 4.linear ]\n",
        "    'NUMBER_OF_PERCEPTRONS_FOR_HIDDEN_LAYERS' : [5]\n",
        "  }\n",
        "\n",
        "PARAMS['NUMBER_OF_PERCEPTRONS_FOR_HIDDEN_LAYERS']\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 679,
      "metadata": {
        "id": "lVlOA3PMs-br"
      },
      "outputs": [],
      "source": [
        "IRIS_MLP = MLP(train[:,0:4] ,train[:,4:7] ,PARAMS )\n",
        "# IRIS_MLP = MLP(train[:,0:4] ,5 * np.reshape(train[:,7],(-1,1,1)) ,PARAMS)\n",
        "# IRIS_MLP = MLP(np.array([[1,1]]) ,np.array([[2,2]]) , PARAMS )\n",
        "# IRIS_MLP = MLP(s1 ,s2, PARAMS )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 680,
      "metadata": {
        "id": "ytZ-BxlTaRr1"
      },
      "outputs": [],
      "source": [
        "# IRIS_MLP.clearAll()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 681,
      "metadata": {
        "id": "Z7dnEwAciqU8"
      },
      "outputs": [],
      "source": [
        "\n",
        "IRIS_MLP.trainingModel(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 683,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPZptHsrv_fL",
        "outputId": "ff8f4c15-995c-4065-f1dd-20c20585b1bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.99848164 0.17198087 0.        ] [1. 0. 0.]\n",
            "[0.99823584 0.17201416 0.        ] [1. 0. 0.]\n",
            "[0.         0.38055866 0.99072584] [0. 0. 1.]\n",
            "[0.99604302 0.17218898 0.        ] [1. 0. 0.]\n",
            "[0.99728197 0.17216161 0.        ] [1. 0. 0.]\n",
            "[0.        0.3026091 0.       ] [0. 1. 0.]\n",
            "[0.         0.33196034 0.17810801] [0. 1. 0.]\n",
            "[0.         0.38240142 1.02152945] [0. 0. 1.]\n",
            "[0.         0.33600394 0.24572055] [0. 1. 0.]\n",
            "[0.98805872 0.17302123 0.        ] [1. 0. 0.]\n",
            "[0.99898129 0.17227874 0.        ] [1. 0. 0.]\n",
            "[0.         0.37917907 0.96763975] [0. 0. 1.]\n",
            "[0.97976824 0.17394422 0.        ] [1. 0. 0.]\n",
            "[0.         0.31213527 0.        ] [0. 1. 0.]\n",
            "[0.99441637 0.17238797 0.        ] [1. 0. 0.]\n",
            "[0.         0.36742499 0.77111898] [0. 0. 1.]\n",
            "[0.         0.33139169 0.16921235] [0. 1. 0.]\n",
            "[0.         0.38264939 1.0256649 ] [0. 0. 1.]\n",
            "[0.         0.3705859  0.82401075] [0. 1. 0.]\n",
            "[0.99665289 0.17220335 0.        ] [1. 0. 0.]\n",
            "[0.         0.38167945 1.00944926] [0. 0. 1.]\n",
            "[0.         0.38050263 0.98977212] [0. 0. 1.]\n",
            "[0.98550346 0.17332274 0.        ] [1. 0. 0.]\n",
            "[0.         0.31379119 0.        ] [0. 1. 0.]\n",
            "[0.         0.31049346 0.        ] [0. 1. 0.]\n",
            "[0.         0.38420586 1.05169178] [0. 0. 1.]\n",
            "[0.99136976 0.17264827 0.        ] [1. 0. 0.]\n",
            "[1.00092891 0.17177891 0.        ] [1. 0. 0.]\n",
            "[0.99829012 0.17209695 0.        ] [1. 0. 0.]\n",
            "[0.         0.37859822 0.95792809] [0. 0. 1.]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(29, 30, '96.66666666666667%')"
            ]
          },
          "execution_count": 683,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "s=0\n",
        "k=0\n",
        "for i in test: #test or train\n",
        "  est = IRIS_MLP.calculateMLP_outputForRow_k(i[0:4])\n",
        "  print(est , i[4:7])\n",
        "  k+=int(np.argmax(est) == np.argmax(i[4:7]) )\n",
        "  s+=1\n",
        "\n",
        "  \n",
        "\n",
        "k, s , str(k/s * 100)+'%'\n",
        "\n",
        " # # print([IRIS_MLP.calculateMLP_outputForRow_k(s1[i])[0] ,IRIS_MLP.calculateMLP_outputForRow_k(s1[i])[1]] , list(s2[i]))\n",
        " # # s+=int([int(IRIS_MLP.calculateMLP_outputForRow_k(s1[i])[0]>0) ,int(IRIS_MLP.calculateMLP_outputForRow_k(s1[i])[1]>0)] == list(s2[i]))\n",
        "# print(IRIS_MLP.calculateMLP_outputForRow_k([1,1]) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 684,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqH9EhAf5w1y",
        "outputId": "41fe2190-620d-4b24-e892-b5da10e6313d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "layer0\n",
            "percepron0\n",
            "0.6745741915769821\n",
            "percepron1\n",
            "0.7228124777241237\n",
            "percepron2\n",
            "0.2240955526384436\n",
            "percepron3\n",
            "0.7604801556986462\n",
            "layer1\n",
            "percepron0\n",
            "-0.2675212637673663\n",
            "-0.7665263801452425\n",
            "0.6370509415846145\n",
            "1.1836632022822173\n",
            "percepron1\n",
            "0.4836541800763478\n",
            "0.6836016974115433\n",
            "0.1282189587269721\n",
            "0.7420572263552115\n",
            "percepron2\n",
            "0.4805091716517456\n",
            "0.7603537062079601\n",
            "0.7408765129602126\n",
            "0.6396585767737258\n",
            "percepron3\n",
            "0.3517275271994939\n",
            "0.6309296244390415\n",
            "0.7935077330435562\n",
            "0.7092022370811348\n",
            "percepron4\n",
            "0.6145790530678699\n",
            "0.060287918595537365\n",
            "0.6798215817599988\n",
            "0.46831036896129835\n",
            "layer2\n",
            "percepron0\n",
            "-1.1161823602646326\n",
            "-0.05089353945017402\n",
            "-0.08852231696908709\n",
            "0.387556356525203\n",
            "-0.3612706469169865\n",
            "percepron1\n",
            "0.10699432331504279\n",
            "0.45662882565550805\n",
            "0.3094868202759435\n",
            "-0.040245292057601134\n",
            "-0.4474039319375628\n",
            "percepron2\n",
            "1.7890402397912282\n",
            "0.3079120254786013\n",
            "-0.5599813758330291\n",
            "0.0028568307193348606\n",
            "-0.4671549026249559\n"
          ]
        }
      ],
      "source": [
        "for i in range(IRIS_MLP.numberOfLayers):\n",
        "  print('layer'+str(i))\n",
        "  for j in range(IRIS_MLP.layers[i].numberOfPerceptrons):\n",
        "    print('percepron'+str(j))\n",
        "    for k in range(IRIS_MLP.layers[i].perceptrons[j].numberOfInputs):\n",
        "      print(IRIS_MLP.layers[i].perceptrons[j].inputBranchs[k].w)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNMfrpIdokfXxCa2NRanczm",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "implementationDynamicMLP.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
